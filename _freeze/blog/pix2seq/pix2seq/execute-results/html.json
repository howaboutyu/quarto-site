{
  "hash": "1bfec0262a5cf9d9b7c4bc8b6a78f1f2",
  "result": {
    "markdown": "---\ntitle: Pix2Seq\ndate: 21 May 2023\nauthor:\n  - Hao Bo Yu\ntitle-block-banner: true\ncode-annotations: hover\nformat: html\nreference-location: margin\nbibliography: ../../references.bib\nhtml-math-method: katex\neditor:\n  render-on-save: true\nexecute:\n  enabled: true\n---\n\nin essense they cast object detection aas a language mdoeling task conditioned on pixel inputs. traditional object detection tasks require lots of different building blocks, such as MaskRCNN, YOLO, SSD, etc. They usually have a backbone network that extracts features from the image, and then a head that predicts the bounding boxes and classes. The design of the head is usually very specific to the task, and the backbone is usually a pretrained network. \n\n\nthis paper turns object detection into a language modeling task, using a encoder-decoder architecture to preidction the bounding boxes as a sequence of tokens. The\n\n## The sequence construction\n\nthe sequence onstruction is taken from two corner points of the bounding box the x and y coordinates. so an object is represented as a sequence of fix tokens $[y_{min}, x_{min}, y_{max}, x_{max}, c]$, where ach of the values are turned into an integer between 1 and $n_{bins}$. \n\n## the objective is \n\nThe objective is trained to predict the tokens given an image and its preceding tokens. \n$$\n\\text{maximize} \\sum_{j=1}^T \\log P(\\tilde{y}_j | y_{1:t-1}, x)\n$$\n\nThis is done by using a transformer encoder-decoder architecture, the encoder can be any feature extractor such as CNNs or transformers. The decoder is a transformer decoder that predicts the next token given the previous tokens.\n\n## sequence augmentation\n\nthey found some issues, that includes:\n\nthe model tends to finish the sequence early without predicting all the objects. they use sequence augmentation where they add some synthetic noise tokens, so that the model can learn to identify the noise tokens rather than to mimic them. \n\n\nthey also add some random bounding boxes as input toe the decoder, and they use the target of noise objects to the \"noise\" class, and disgard the weights of the noise class to be zero.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\ndataset = tfds.load( \"coco/2017\", split=[\"validation\"], with_info=True, data_dir=\"~/data\" )\n\nprint(dataset)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n2024-01-22 23:30:58.138176: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nWARNING:absl:You use TensorFlow DType <dtype: 'int64'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to int64.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n([<_PrefetchDataset element_spec={'image': TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name=None), 'image/filename': TensorSpec(shape=(), dtype=tf.string, name=None), 'image/id': TensorSpec(shape=(), dtype=tf.int64, name=None), 'objects': {'area': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'bbox': TensorSpec(shape=(None, 4), dtype=tf.float32, name=None), 'id': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'is_crowd': TensorSpec(shape=(None,), dtype=tf.bool, name=None), 'label': TensorSpec(shape=(None,), dtype=tf.int64, name=None)}}>], tfds.core.DatasetInfo(\n    name='coco',\n    full_name='coco/2017/1.1.0',\n    description=\"\"\"\n    COCO is a large-scale object detection, segmentation, and\n    captioning dataset.\n    \n    Note:\n     * Some images from the train and validation sets don't have annotations.\n     * Coco 2014 and 2017 uses the same images, but different train/val/test splits\n     * The test split don't have any annotations (only images).\n     * Coco defines 91 classes but the data only uses 80 classes.\n     * Panotptic annotations defines defines 200 classes but only uses 133.\n    \"\"\",\n    config_description=\"\"\"\n    \n    This version contains images, bounding boxes and labels for the 2017 version.\n    \n    \"\"\",\n    homepage='http://cocodataset.org/#home',\n    data_path='/Users/david/data/coco/2017/1.1.0',\n    file_format=tfrecord,\n    download_size=25.20 GiB,\n    dataset_size=24.98 GiB,\n    features=FeaturesDict({\n        'image': Image(shape=(None, None, 3), dtype=uint8),\n        'image/filename': Text(shape=(), dtype=string),\n        'image/id': int64,\n        'objects': Sequence({\n            'area': int64,\n            'bbox': BBoxFeature(shape=(4,), dtype=float32),\n            'id': int64,\n            'is_crowd': bool,\n            'label': ClassLabel(shape=(), dtype=int64, num_classes=80),\n        }),\n    }),\n    supervised_keys=None,\n    disable_shuffling=False,\n    splits={\n        'test': <SplitInfo num_examples=40670, num_shards=64>,\n        'train': <SplitInfo num_examples=118287, num_shards=256>,\n        'validation': <SplitInfo num_examples=5000, num_shards=8>,\n    },\n    citation=\"\"\"@article{DBLP:journals/corr/LinMBHPRDZ14,\n      author    = {Tsung{-}Yi Lin and\n                   Michael Maire and\n                   Serge J. Belongie and\n                   Lubomir D. Bourdev and\n                   Ross B. Girshick and\n                   James Hays and\n                   Pietro Perona and\n                   Deva Ramanan and\n                   Piotr Doll{'{a}}r and\n                   C. Lawrence Zitnick},\n      title     = {Microsoft {COCO:} Common Objects in Context},\n      journal   = {CoRR},\n      volume    = {abs/1405.0312},\n      year      = {2014},\n      url       = {http://arxiv.org/abs/1405.0312},\n      archivePrefix = {arXiv},\n      eprint    = {1405.0312},\n      timestamp = {Mon, 13 Aug 2018 16:48:13 +0200},\n      biburl    = {https://dblp.org/rec/bib/journals/corr/LinMBHPRDZ14},\n      bibsource = {dblp computer science bibliography, https://dblp.org}\n    }\"\"\",\n))\n```\n:::\n:::\n\n\n",
    "supporting": [
      "pix2seq_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}