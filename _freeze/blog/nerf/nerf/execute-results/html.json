{
  "hash": "9003e9563f90453924ca8f3329aa8841",
  "result": {
    "markdown": "---\ntitle: 'Learning: NeRF + JAX '\ndate: 03 April 2023\nauthor:\n  - Hao Bo Yu\ntitle-block-banner: true\ncode-annotations: hover\nformat: html\nreference-location: margin\nbibliography: ../../references.bib\nhtml-math-method: katex\nhighlight-style: solarized\ncallout-appearance: simple\ncomments:\n  giscus:\n    repo: howaboutyu/quarto-site\nreading-time: true\ncategories:\n  - machine learning\n  - jax\n  - nerf\ndescription: This article discusses the implementation of the NeRF algorithm using the Jax programming language.\n---\n\n## Introduction\n\nWhen I first read the NeRF paper, I was amazed by its elegance and its potential for scene representation using neural networks. I also saw it as an opportunity to learn more about JAX. In this article, I will outline the NeRF algorithm and explain some of the things I've learned about Jax.  \n\n\n\n\n{{< video https://youtu.be/0puPLaTeO60  >}}\n\n\n\nIn the 5-second video above, you can see the output of the NeRF algorithm [@Mildenhall2020] that I implemented in Jax [@jax2018github], which was trained on 35 sparse images. But how was this achieved?\n\n::: {.callout-note }\nCheck out the accompanying code\n[here](https://github.com/higgsboost/nerf-jax)\n\n:::\n\nNeRF achieves this by integrating rays of light that emanate from the camera and intersect the object. In other words, NeRF can use a bunch of rays to create an image of the object from different viewpoints. NeRF uses a multi-layered perception (MLP) that can be trained to represent a scene from a set of 2D images, and can then generate images taken from new angles. That is how it was done!\n\n\n\n\n## The NeRF Neural Network \n\nThe NeRF model represents a scene as a neural network, where at each 3D position $\\vec{x} \\in \\mathbb{R}^3$ and viewing direction $\\vec{d} \\in \\mathbb{R}^3$, the output of the model is the color (RGB) $\\vec{c}$ and the density $\\sigma$ for that point in space. Thus, we have:\n\n$$\nF_\\Theta : ( \\vec{x}, \\vec{d}) \\rightarrow (\\vec{c}, \\sigma)\n$$\n\nThe network $F_\\Theta$ is a Multi-Layer Perceptron (MLP); the network is quite simple, as illustrated in Figure 1. It consists of a series of linear layers with ReLU activations.\n\n![\nThe NeRF model figure is taken from @Mildenhall2020. The blue blocks represent MLP layers. Black arrows indicate the ReLU activation layer, the orange arrow indicates no activation and the dashed black line indicates sigmoid activation.\n](nerf_model.png \"Title: The NeRF model\"){#fig-nerf-model}\n\n\n\n<!-- \n\n### Camera ray and position encoding\n#### Camera ray \n\nBefore training the network, we need to generate the rays used for training. These rays are generated from the camera, which is defined by extrinsic parameters, such as rotation and translation matrices - $[R | T]$, focal length, and near/far distances. The origin, $\\vec{o}$, corresponds to the camera translation vector for that image.\n\nThe vector $\\vec{d}$ can be thought of as $\\vec{d} = R\\vec{d}_o$, where $\\vec{d}_o$ is the direction vector $\\vec{d}_o = [x, y, -f]$ using the [pin-hole camera model](https://en.wikipedia.org/wiki/Pinhole_camera_model). The magnitude doesn't matter, so $\\vec{d}_o = [x/f, y/f, -1]$.\n\n\n#### Positional encoding\n\nPositional encoding is a straightforward method for encoding the 3D position, $\\vec{x}$, into a vector. The authors used sine and cosine functions to encode the position, enabling the MLP to capture higher frequency functions. The encoding is given by:\n\n$$\n\\gamma(x) = (sin(2^0\\pi x), cos(2^0 x), \\dots, sin(2^{L-1}\\pi x), cos(2^{L-1} x))\n$$\n\nHere, $x$ can be either the position or the direction, and $L$ is the number of frequencies. The authors used $L_{position}=10$ and $L_{direction}=4$. -->\n\n#### Model in JAX\n\nUsing [Flax](https://github.com/google/flax), we can easily define the model in JAX. The code is shown below:\n\n::: {.callout-tip }\n\nHover over the code annotations (like ① ) for further explanation.\n\n:::\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport jax\nimport flax.linen as nn\nimport jax.numpy as jnp\n\nclass Model(nn.Module):\n\n  @nn.compact\n  def __call__(self, position, direction):\n    x = position\n\n    for i in range(7):\n        x = nn.Dense(256, name=f\"layer_{i}\")(x)\n        x = nn.relu(x)\n\n        # Concatenate x with original input\n        if i == 4:\n            x = jnp.concatenate([x, position], -1)\n\n    x = nn.Dense(256, name=\"layer_7\")(x)\n\n    vol_density = nn.Dense(1, name=\"layer_8\")(x)\n\n    # Create an output for the volume density that is view-independent\n    # and > 0 by using a ReLU activation function \n    vol_density = jax.nn.relu(vol_density)\n\n    # Concatenate direction information after the volume density\n    x = jnp.concatenate([x, direction], -1)\n    x = nn.Dense(128, name=\"layer_9\")(x)\n    x = nn.relu(x)\n    x = nn.Dense(3, name=\"layer_10\")(x)\n\n    # Create an output for the RGB color and make sure it is in the range [0, 1] \n    rgb = nn.sigmoid(x)\n    return rgb, vol_density\n\nL_position = 10 \nL_direction = 4 \ndummy_pos = jnp.ones((1, L_position * 6 + 3))\ndummy_dir = jnp.ones((1, L_direction * 6 + 3))\n\nmodel = Model()\n\nparams = model.init( # <1>\n    jax.random.PRNGKey(0), # <1>\n    dummy_pos, # <1>\n    dummy_dir # <1>\n) # <1>\n\nprint(\n  Model().tabulate( # <2>\n    jax.random.PRNGKey(0),  # <2>\n    dummy_pos,  # <2> \n    dummy_dir # <2>\n  ) \n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n                                 Model Summary                                  \n┏━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ path     ┃ module ┃ inputs          ┃ outputs        ┃ params                ┃\n┡━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━┩\n│          │ Model  │ - float32[1,63] │ - float32[1,3] │                       │\n│          │        │ - float32[1,27] │ - float32[1,1] │                       │\n├──────────┼────────┼─────────────────┼────────────────┼───────────────────────┤\n│ layer_0  │ Dense  │ float32[1,63]   │ float32[1,256] │ bias: float32[256]    │\n│          │        │                 │                │ kernel:               │\n│          │        │                 │                │ float32[63,256]       │\n│          │        │                 │                │                       │\n│          │        │                 │                │ 16,384 (65.5 KB)      │\n├──────────┼────────┼─────────────────┼────────────────┼───────────────────────┤\n│ layer_1  │ Dense  │ float32[1,256]  │ float32[1,256] │ bias: float32[256]    │\n│          │        │                 │                │ kernel:               │\n│          │        │                 │                │ float32[256,256]      │\n│          │        │                 │                │                       │\n│          │        │                 │                │ 65,792 (263.2 KB)     │\n├──────────┼────────┼─────────────────┼────────────────┼───────────────────────┤\n│ layer_2  │ Dense  │ float32[1,256]  │ float32[1,256] │ bias: float32[256]    │\n│          │        │                 │                │ kernel:               │\n│          │        │                 │                │ float32[256,256]      │\n│          │        │                 │                │                       │\n│          │        │                 │                │ 65,792 (263.2 KB)     │\n├──────────┼────────┼─────────────────┼────────────────┼───────────────────────┤\n│ layer_3  │ Dense  │ float32[1,256]  │ float32[1,256] │ bias: float32[256]    │\n│          │        │                 │                │ kernel:               │\n│          │        │                 │                │ float32[256,256]      │\n│          │        │                 │                │                       │\n│          │        │                 │                │ 65,792 (263.2 KB)     │\n├──────────┼────────┼─────────────────┼────────────────┼───────────────────────┤\n│ layer_4  │ Dense  │ float32[1,256]  │ float32[1,256] │ bias: float32[256]    │\n│          │        │                 │                │ kernel:               │\n│          │        │                 │                │ float32[256,256]      │\n│          │        │                 │                │                       │\n│          │        │                 │                │ 65,792 (263.2 KB)     │\n├──────────┼────────┼─────────────────┼────────────────┼───────────────────────┤\n│ layer_5  │ Dense  │ float32[1,319]  │ float32[1,256] │ bias: float32[256]    │\n│          │        │                 │                │ kernel:               │\n│          │        │                 │                │ float32[319,256]      │\n│          │        │                 │                │                       │\n│          │        │                 │                │ 81,920 (327.7 KB)     │\n├──────────┼────────┼─────────────────┼────────────────┼───────────────────────┤\n│ layer_6  │ Dense  │ float32[1,256]  │ float32[1,256] │ bias: float32[256]    │\n│          │        │                 │                │ kernel:               │\n│          │        │                 │                │ float32[256,256]      │\n│          │        │                 │                │                       │\n│          │        │                 │                │ 65,792 (263.2 KB)     │\n├──────────┼────────┼─────────────────┼────────────────┼───────────────────────┤\n│ layer_7  │ Dense  │ float32[1,256]  │ float32[1,256] │ bias: float32[256]    │\n│          │        │                 │                │ kernel:               │\n│          │        │                 │                │ float32[256,256]      │\n│          │        │                 │                │                       │\n│          │        │                 │                │ 65,792 (263.2 KB)     │\n├──────────┼────────┼─────────────────┼────────────────┼───────────────────────┤\n│ layer_8  │ Dense  │ float32[1,256]  │ float32[1,1]   │ bias: float32[1]      │\n│          │        │                 │                │ kernel:               │\n│          │        │                 │                │ float32[256,1]        │\n│          │        │                 │                │                       │\n│          │        │                 │                │ 257 (1.0 KB)          │\n├──────────┼────────┼─────────────────┼────────────────┼───────────────────────┤\n│ layer_9  │ Dense  │ float32[1,283]  │ float32[1,128] │ bias: float32[128]    │\n│          │        │                 │                │ kernel:               │\n│          │        │                 │                │ float32[283,128]      │\n│          │        │                 │                │                       │\n│          │        │                 │                │ 36,352 (145.4 KB)     │\n├──────────┼────────┼─────────────────┼────────────────┼───────────────────────┤\n│ layer_10 │ Dense  │ float32[1,128]  │ float32[1,3]   │ bias: float32[3]      │\n│          │        │                 │                │ kernel:               │\n│          │        │                 │                │ float32[128,3]        │\n│          │        │                 │                │                       │\n│          │        │                 │                │ 387 (1.5 KB)          │\n├──────────┼────────┼─────────────────┼────────────────┼───────────────────────┤\n│          │        │                 │          Total │ 530,052 (2.1 MB)      │\n└──────────┴────────┴─────────────────┴────────────────┴───────────────────────┘\n                                                                                \n                       Total Parameters: 530,052 (2.1 MB)                       \n\n\n```\n:::\n:::\n\n\n1. We can use the `init` method to initialize the model parameters. [docs](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html) \n2. We can use the `tabulate` method to see the model summary. [docs](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html)\n\n\n\n### The NeRF algorithm \n\nConsider a line or ray directed pointing away from the camera, with the equation $\\vec{r}(t) = \\vec{o} +t \\vec{d}$, where $\\vec{o}$ is the origin point on the ray, $t$ is a parameter that controls how far along the ray we are and $\\vec{d}$ is the direction of the ray - a unit vector pointing away from the camera. The authors split the ray into $N$ segments, with each segment having a length equal to $\\delta_i = t_{i+1}-t_i$. At each segment along the ray, the color $c_i$ and volume density $\\sigma_i$ of segment $i$ can be approximated by the neural network $F_\\Theta(\\vec{r}(t_i), \\vec{d_i}) \\rightarrow (\\vec{c}_i, \\sigma_i)$. The expected color of that ray that would appear in our image, $\\hat{C}(\\vec{r})$, can be calculated using a set of approximation integral equations, as shown below:\n\n$$\n\\hat{C}(\\vec{r}) = \\sum_{i=1}^{N} T_i a_i \\vec{c}_i \n$${#eq-nerf-color}\n\nwhere, \n\n$$\na_i = (1- \\text{exp}(-\\sigma_i \\delta_i))\n$${#eq-nerf-alpha}\n\n\nand,\n$$\nT_i=\\text{exp}\\left( -\\sum_{j=1}^{i-1} \\sigma_j \\delta_j \\right)\n$${#eq-nerf-transmittance}\n\nOkay, so what do these equations mean? Let's start with the color equation. The color of the ray is the sum of the color of each segment along the ray, weighted by the transmittance $T_i$ and the alpha value $a_i$. The alpha value $a_i$ is the probability that the ray has not encountered anything up to that point. The transmittance $T_i$ is the accumulated transmittance up to that point, which can be thought of as the probability that the ray has not encountered anything up to that point. Therefore, $T_i$ should be $1$ for every segment along the ray before a solid object. If the ray encounters a solid object, $T_i$ would start to decrease because the volume density $\\sigma$ begins to increase. We can observe that the alpha values $a_i$ for low volume density become $0$, ignoring that color in the integral. It's an intriguing relationship!\n\n\n::: {.callout-info}\nThe authors designed the network to ensure that the volume density $\\sigma$ is view-independent ($\\partial{\\sigma} / \\partial{\\vec{d}} = 0$) by setting the directional input $\\vec{d}$ after the $\\sigma$ output.\n:::\n\n\nThe equations above can be implemented using `jax.numpy` ([docs](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html)), a JAX version of numpy, as follows:\n\n```python\nrgb = jax.nn.sigmoid(rgb)\ndensity = jax.nn.relu(density)\nt_delta = t[..., 1:] - t[..., :-1]\n\nT_i = jnp.cumsum(jnp.squeeze(density) * t_delta, -1) # <3>\nT_i = jnp.insert(T_i, 0, jnp.zeros_like(T_i[..., 0]), -1) # <3>\nT_i = jnp.exp(-T_i)[..., :-1] # <3>\n\na_i = 1.0 - jnp.exp(-density * t_delta[..., jnp.newaxis]) # <2>\n\nweights = T_i[..., jnp.newaxis] * a_i # <1>\nc_array = weights * rgb # <1>\nc_sum = jnp.sum(c_array, -2) # <1>\n```\n\n\n1. @eq-nerf-color\n2. @eq-nerf-alpha\n3. @eq-nerf-transmittance\n\n## Hierarchical volume sampling (HVS)\n\nFree space and solid objects do not contribute to the final color. Therefore, we can improve efficiency by sampling the volume more densely around points along the ray that have a larger weight, which typically indicates the surfaces of objects. By defining the weight as $w_i = T_i a_i$, we can rewrite @eq-nerf-color as:\n\n$$\n\\hat{C}(\\vec{r}) = \\sum_{i=1}^{N} w_i \\vec{c}_i\n$$\n\nThe authors employ a two-step approach for processing the model. First, it uses a coarse set of points with $N_c$ points. Subsequently, it employs a finer set of $N_f$ points to sample the relevant parts of the volume with denser sampling. To sample these points, the method uses inverse transform sampling based on a probability density function (PDF) created using the expression $w_i = T_i(1-\\exp(-\\sigma_i \\delta_i))$. \n\nIn essence, this method involves the use of a random variable $X$ that represents the element along the ray that is the first surface the ray hits. To ensure correct sampling distribution, the PDF is normalized as follows:\n\n$$\nP(X=x) = \\frac{w_x}{  \\sum_j^{N_c} w_j } \\quad \\text{for} \\quad 1 < x < N_c \n$$\n\n### Inverse transform sampling \n\nConsider a ray taken from an arbitrary NeRF model. The Probability Density Function (PDF), $P(x)$, the Cumulative Distribution Function (CDF)^[The CDF of a random variable $X$ is $F(x) = P(X\\leq x)$], $F(x)$, and the inverse CDF, $F^{-1}(x)$, are plotted below. From the plots, it can be inferred that there is an object surface near the $52^{\\text{th}}$ element along the ray. \n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\"}\n''' \nPDF, CDF, and inverse CDF of the weights\n'''\nimport numpy as np\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\n\nweights = np.squeeze(np.load('weights.npy'))\n\n\nk = np.sum(weights) \n# normalize\nweights = weights/k\ndx = 1. \nx=np.arange(0, 128)\n\ncdf = np.cumsum(weights * dx)\n\n\nfig = make_subplots(\n  rows=2, \n  cols=2, \n  subplot_titles=(\"PDF\", \"CDF\", \"Inverse CDF\"),\n\tspecs=[[{}, {}], [{\"colspan\": 2}, None]],\n)\n\nfig.add_trace(go.Scatter(x=x, y=weights, name='PDF'), row=1, col=1)\nfig.add_trace(go.Scatter(x=x, y=cdf, name='CDF'), row=1, col=2)\n\n# plot inverse CDF\nfig.add_trace(go.Scatter(x=cdf, y=x, name='inverse CDF'), row=2, col=1)\n\n\n# set x label to be i\nfig.update_xaxes(title_text=\"i\", row=1, col=1)\nfig.update_xaxes(title_text=\"i\", row=1, col=2)\nfig.update_xaxes(title_text=\"probability\", row=2, col=1)\n\n\n# set y label to be probability\nfig.update_yaxes(title_text=\"probability\", row=1, col=1)\nfig.update_yaxes(title_text=\"i\", row=2, col=1)\n\n\n# remove legend\nfig.update_layout(showlegend=False)\n\nfig.show()\n\n# area\narea = np.sum(weights * dx)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>                            <div id=\"13a5f594-1a65-4609-9434-f0eb36bb1775\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"13a5f594-1a65-4609-9434-f0eb36bb1775\")) {                    Plotly.newPlot(                        \"13a5f594-1a65-4609-9434-f0eb36bb1775\",                        [{\"name\":\"PDF\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127],\"y\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0073016295,0.04691421,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0040191794,0.28869635,0.30694568,0.24993263,0.08067956,0.014787792,0.00015746054,0.00031231507,0.00006540788,0.00009615444,0.00005168686,0.00001563884,0.000007708381,0.000013950369,9.724471e-7,7.549145e-7,5.8900747e-7,6.65698e-8,6.460915e-8,7.584686e-8,7.7289855e-9,5.8286775e-10,9.889345e-9,7.210577e-9,4.897314e-9,3.6558045e-8,7.9649226e-8,1.3151363e-8,4.8314037e-9,5.2826454e-9,2.3088084e-10,1.2965623e-11,3.3660767e-12,1.2093697e-13,6.671051e-15,3.7607146e-16,2.936182e-17,1.703752e-17,4.3149556e-17,2.5653496e-17,4.1968626e-18,1.1263013e-17,8.9889585e-18,7.764315e-18,1.224687e-17,3.6724954e-18,6.025278e-19,1.8905968e-19,8.6103447e-20,9.5140466e-20,1.4411517e-20,6.505582e-21,7.5175685e-21,1.6889629e-20,8.535188e-21,1.2683265e-21,1.6871615e-21,4.0877517e-21,9.770115e-22,6.010995e-22,4.2120909e-22,5.907191e-22,1.3263757e-21,7.535066e-22,1.9875213e-21,6.4763763e-21,2.5782664e-20,1.6645615e-21,5.5148663e-22,2.1992921e-23,1.9723886e-23,4.5095044e-24,1.1766907e-24,3.0024741e-25,3.0887867e-26,7.5982676e-26,4.461893e-25,1.5193641e-25],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"name\":\"CDF\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127],\"y\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0073016295,0.054215837,0.054215837,0.054215837,0.054215837,0.054215837,0.054215837,0.054215837,0.054215837,0.054215837,0.054215837,0.054215837,0.058235016,0.34693137,0.653877,0.90380967,0.9844892,0.999277,0.9994345,0.9997468,0.9998122,0.9999083,0.99996,0.9999756,0.9999833,0.99999726,0.9999982,0.999999,0.9999996,0.99999964,0.9999997,0.99999976,0.99999976,0.99999976,0.99999976,0.99999976,0.99999976,0.9999998,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"name\":\"inverse CDF\",\"x\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0073016295,0.054215837,0.054215837,0.054215837,0.054215837,0.054215837,0.054215837,0.054215837,0.054215837,0.054215837,0.054215837,0.054215837,0.058235016,0.34693137,0.653877,0.90380967,0.9844892,0.999277,0.9994345,0.9997468,0.9998122,0.9999083,0.99996,0.9999756,0.9999833,0.99999726,0.9999982,0.999999,0.9999996,0.99999964,0.9999997,0.99999976,0.99999976,0.99999976,0.99999976,0.99999976,0.99999976,0.9999998,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999,0.9999999],\"y\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45],\"title\":{\"text\":\"i\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.625,1.0],\"title\":{\"text\":\"probability\"}},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0],\"title\":{\"text\":\"i\"}},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.625,1.0]},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"probability\"}},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.0,0.375],\"title\":{\"text\":\"i\"}},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"PDF\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"CDF\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Inverse CDF\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.375,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('13a5f594-1a65-4609-9434-f0eb36bb1775');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>\n```\n:::\n:::\n\n\nInverse transform sampling involves sampling from a uniform distribution $U(0,1)$ and then using the inverse CDF to find the corresponding value in the original distribution. Observing the inverse CDF, when $Z\\sim U(0,1)$, we can see that most values are close to $i=52$. This observation aligns with the PDF, which exhibits the highest probability near that point along the ray.\n\nTo predict the final color, we use a combination of fine and coarse points, which are randomly sampled from specific regions of the distribution. Shown below, the distribution of fine points is heavily centered around the $52^{\\text{th}}$ segment, while the coarse points are evenly spread out over the distribution.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\"}\nimport jax.numpy as jnp \nimport jax\n\nZ = jax.random.uniform(key=jax.random.PRNGKey(2), shape=[128])\nt_to_sample = jnp.arange(128)\ncdf = jnp.array(cdf)\n\ndef inverse_sample(Z, cdf, t_to_sample):\n    \"\"\"\n    Samples from the inverse CDF using the inverse transform sampling method.\n    Inputs:\n        Z (jnp.ndarray): Random numbers from a uniform distribution U(0, 1). Shape: (batch_size, num_to_sample)\n        cdf (jnp.ndarray): The CDF of the distribution to sample from. Shape: (batch_size, num_samples)\n        t_to_sample (jnp.ndarray): The points to sample from the distribution. Shape: (batch_size, num_fine_samples)\n\n    Outputs:\n        sampled_t (jnp.ndarray): The sampled points from the distribution. Shape: (batch_size, num_to_sample)\n\n    Where num_to_sample = Z.shape[1]\n    \"\"\"\n    abs_diff = jnp.abs(cdf[..., jnp.newaxis, :] - Z[..., jnp.newaxis])\n\n    argmin = jnp.argmin(abs_diff, 1)\n\n    sampled_t = jnp.take_along_axis(t_to_sample, argmin, 0)\n\n    return sampled_t\n\n\nfine_points = inverse_sample(Z, cdf, t_to_sample)\n\ncoarse_points = jnp.linspace(0, 128, 64)\n\n\ntrace1 = go.Histogram(x=coarse_points, nbinsx=64, name='Coarse Points', marker=dict(color='#008080'))\ntrace2 = go.Histogram(x=fine_points, nbinsx=64, name='Fine Points', marker=dict(color='#FFA500'))\n\ndata = [trace1, trace2]\nlayout = go.Layout(title='Histogram of Coarse and Fine Points for HVS Sampling', xaxis=dict(title='Value'), yaxis=dict(title='Count'))\nfig = go.Figure(data=data, layout=layout)\n\n\nfig.show()\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>                            <div id=\"684a282a-5b68-4848-a514-6e6b516b240b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"684a282a-5b68-4848-a514-6e6b516b240b\")) {                    Plotly.newPlot(                        \"684a282a-5b68-4848-a514-6e6b516b240b\",                        [{\"marker\":{\"color\":\"#008080\"},\"name\":\"Coarse Points\",\"nbinsx\":64,\"x\":[0.0,2.0317461,4.0634923,6.0952387,8.126985,10.1587305,12.190477,14.222223,16.25397,18.285715,20.317461,22.349207,24.380955,26.4127,28.444447,30.476192,32.50794,34.539684,36.57143,38.603176,40.634922,42.666668,44.698414,46.73016,48.76191,50.793655,52.8254,54.857147,56.888893,58.92064,60.952385,62.98413,65.01588,67.04762,69.07937,71.111115,73.14286,75.17461,77.20635,79.2381,81.269844,83.30159,85.333336,87.36508,89.39683,91.42857,93.46032,95.492065,97.52382,99.555565,101.58731,103.61906,105.6508,107.68255,109.714294,111.74604,113.77779,115.80953,117.84128,119.873024,121.90477,123.936516,125.96826,128.0],\"type\":\"histogram\"},{\"marker\":{\"color\":\"#FFA500\"},\"name\":\"Fine Points\",\"nbinsx\":64,\"x\":[51,51,51,53,52,52,53,54,51,52,50,51,50,51,53,53,53,50,52,53,52,52,51,52,50,52,52,50,52,54,50,39,51,52,52,53,53,53,51,51,51,52,52,50,52,51,52,54,52,52,52,53,50,52,52,51,51,52,51,52,52,51,51,52,52,52,53,51,53,52,51,51,53,51,54,52,38,50,51,53,51,38,51,53,50,51,52,52,39,52,54,52,54,39,51,52,54,52,51,54,39,51,54,51,52,51,38,51,53,53,52,55,50,52,50,50,50,52,51,51,53,50,51,52,52,51,52,51],\"type\":\"histogram\"}],                        {\"title\":{\"text\":\"Histogram of Coarse and Fine Points for HVS Sampling\"},\"xaxis\":{\"title\":{\"text\":\"Value\"}},\"yaxis\":{\"title\":{\"text\":\"Count\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('684a282a-5b68-4848-a514-6e6b516b240b');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>\n```\n:::\n:::\n\n\n## Jax Learnings \n\nI utilized `Jax` in combination with `flax` and `optax` for this project.\n\n* [Jax](https://github.com/google/jax): Jax provides automatic differentiation and XLA capabilities for flexible and efficient computation. \n \n* [Flax](https://github.com/google/flax): A neural network library and ecosystem for JAX designed for flexibility - Flax offers a powerful and flexible library to build and train neural networks using Jax. \n\n* [Optax](https://github.com/deepmind/optax):  A gradient processing and optimization library for JAX - Optax simplifies the process of gradient optimization and updating for Jax models.\n\n### The training loop\n\n#### The training state\n\nDuring optimization, we need to keep track of some states, like model parameters and optimizer states. Fortunately, by using `Flax`'s   `TrainState` class, we can conveniently create a training state object that manages the parameter and optimizer (`optax`) states:\n\n```python\nimport optax\nfrom flax.training import checkpoints, train_state\n\n# create learning rate\nlearning_rate_schedule = optax.cosine_decay_schedule( # <1> \n    init_value=config.learning_rate, decay_steps=config.num_epochs * steps_per_epoch # <1>\n) # <1>\n\n# create train state\ntx = optax.adam(learning_rate=learning_rate_schedule) # <2>\nstate = train_state.TrainState.create(apply_fn=model, params=params, tx=tx) # <2>\n\n# replicate the state to all devices \nstate = flax.jax_utils.replicate(state) # <3> \n```\n\n1. Define the learning rate schedule, [see more](https://optax.readthedocs.io/en/latest/api.html).\n2. Create the train state object with `train_state.TrainState.create`, specifying the model, parameters, and optimizer.\n3. If we are using multiple devices for training, use `flax.jax_utils.replicate` to replicate the state to multiple devices. Use the opposite `flax.jax_utils.unreplicate` to get the state for a single device. \n\n\n\n#### The training step \n\nThe following code implements the training step for NeRF using the single-program, multiple-data (SPMD) technique, which allows for parallel computation of the forward pass of a neural network on different input data across different devices (e.g., TPUs).\n\nTo split the batch into sub-batches and have each device perform a sub-batch, we can use `jax.pmap`. Each device has a copy of the model, and we use `pmean` to combine values - such as gradients and losses - from all devices; `jax.lax.pmean(gradients, \"batch\")`.\n\n```python\ndef train_step(state, key, origins, directions, rgbs, nerf_func, use_hvs):\n  \"\"\"\n  Train step\n  Inputs:\n      state: train state\n      key: random key\n      origins: origins of rays [num_devices, batch_size, 3]\n      directions: directions of rays [num_devices, batch_size, 3]\n      rgbs: rgb values of rays [num_devices, batch_size, 3]\n      nerf_func: a function performs the nerf algorithm\n      use_hvs: whether to use hvs\n  Outputs:\n      state: updated train state\n      loss: loss value\n      rgb_pred: predicted rgb values\n      weights: weights of the rays\n      ts: parametric values of the ray\n  \"\"\"\n\n  def loss_func(params):\n      (rendered, rendered_hvs), weights, ts = nerf_func(\n          params=params,\n          model_func=state.apply_fn,\n          key=key,\n          origins=origins,\n          directions=directions,\n      )\n\n      loss = jnp.mean(jnp.square(rendered - rgbs)) # <0> \n\n      if use_hvs:\n          loss += jnp.mean(jnp.square(rendered_hvs - rgbs)) # <0> \n\n      return loss, (rendered, weights, ts)\n\n  # compute loss and grads\n  (loss, (rgbs_pred, weights, ts)), grads = jax.value_and_grad(\n      loss_func, has_aux=True\n  )(state.params)\n\n  # combine grads and loss from all devices\n  grads = jax.lax.pmean(grads, \"batch\") # <1>\n  loss = jax.lax.pmean(loss, \"batch\") # <1>\n\n  # apply updates on the combined grads\n  state = state.apply_gradients(grads=grads) # <2>\n\n  return state, loss, rgbs_pred, weights, ts\n\np_train_step = jax.pmap( # <3>\n    functools.partial(train_step, nerf_func=nerf_func, use_hvs=config.use_hvs), # <3>\n    axis_name=\"batch\", # <3>\n) # <3>\n\nstate = flax.jax_utils.replicate(state) # <4>\n\n...\n\nstate, loss, rgb_pred, weights, ts = p_train_step( # <5>\n    state, key, origins, directions, rgbs\n) # <5>\n```\n0. The loss is computed by comparing the predicted rgb values with the ground truth rgb values for both the coarse and fine renderings. \n1. Use `jax.pmean` to combine the loss and gradients from all devices\n2. Update the state by computing gradients and applying them using the `state.apply_gradients()` method.\n3. Use `jax.pmap` to parallelize the training step across devices  \n4. If we are using multiple devices for training, use `flax.jax_utils.replicate` to replicate the state to multiple devices. Use the opposite `flax.jax_utils.unreplicate` to get the state for a single device.  \n5. Run the training step \n\n\n::: {.callout-tip}\n# Breakpoints \nDuring debugging, we can use `pdb` to set breakpoints. But, `pdb` does not work well with Jax. Instead, we can use `jax.debug.breakpoint()` to set breakpoints. \n\n```python\njax.debug.breakpoint()\n```\n\n:::\n\n\n## Training details \n\nThe model was trained for 400,000 steps, and the learning rate was decayed using a cosine decay schedule. The model was trained on a single A10 GPU (24 GB memory) with a batch size of 2048, the training took about 10 hours.\n\n\n### Training images \nBelow are the training images used for training the model, in total, there are 35 images, 33 of which are used for training and 2 for validation. The images were taken from an iPhone video of a Ramen restaurant Lego set.  \n\n::: {#fig-train }\n![Training images](images/training_img.png){#fig-train-img}\n\nTraining images, 35 images in total.\n:::\n\n\n###  Results\n\nAfter training for around 400k steps, the model was able to achieve a validation [SSIM](https://en.wikipedia.org/wiki/Structural_similarity) score of 0.928.\n\n\nThe following figure shows the ground truth images, the predicted images, and the predicted depth maps. The last column shows the rendered validation images as the model is being trained.\n\n\n::: {#fig-val layout-ncol=4}\n\n![Ground truth image](images/gt_train.png){#fig-gt}\n\n![Predicted image](images/pred_img.png){#fig-pred}\n\n![Predicted depth](images/pred_depth.png){#fig-pred-depth}\n\n![Video of rendered validation images](training.mp4){#fig-rendered} \n\nGround truth images, predicted images, predicted depth maps and rendered validation images.\n\n:::\n\n## Conclusion\n\n\nIn this project, I implemented NeRF using Jax and Flax. I also trained a NeRF model on a custom set of images and rendered a custom 5-second video using NeRF. \n\nLearning about NeRF was a lot of fun, and I hope you enjoyed reading this post as much as I enjoyed writing it. Follow me on [Twitter](https://twitter.com/hyu754) for more updates on my projects.\n\n### What's next?\nNerf has exploded in popularity in the last few years, and there are many implementations and extensions of it. Here are some of the extensions that I found and would like to try out:  \n\n* [KiloNeRF](https://arxiv.org/pdf/2103.13744.pdf): This paper proposes a real-time rendering approach that uses thousands of smaller MLPs to represent parts of a scene, resulting in faster evaluations compared to a single large MLP. \n\n* [NeuS](https://lingjie0206.github.io/papers/NeuS/): This paper introduces NeuS, a novel neural surface reconstruction method that represents the surface as the zero-level set of a signed distance function (SDF).  \n\n\n\n### Additional Resources\n\nIf you want to learn more about NeRF and see additional implementations, here are some helpful resources:\n\n* **Awesome NeRF**: Check out the [awesome NeRF list](https://github.com/awesome-NeRF/awesome-NeRF) for a list of papers, implementations, and resources related to NeRF.\n\n* **Original NeRF Implementation in TensorFlow**: The [original NeRF implementation in TensorFlow](https://github.com/bmild/nerf) by Ben Mildenhall is a great starting point for understanding the NeRF algorithm and its implementation details.\n\n* **NeRF in JAX**: The [NeRF implementation in JAX](https://github.com/google-research/google-research/tree/master/jaxnerf) by the Google Research team is a powerful and efficient implementation of NeRF using JAX and Flax.\n\n* **Flax ImageNet Example**: The [Flax ImageNet example](https://github.com/google/flax/tree/main/examples/imagenet) provides a great introduction to using Flax for large-scale deep learning tasks. This example demonstrates how to train a state-of-the-art image classification model on the ImageNet dataset using Flax.\n\n",
    "supporting": [
      "nerf_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n<script type=\"text/javascript\">\nwindow.PlotlyConfig = {MathJaxConfig: 'local'};\nif (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\nif (typeof require !== 'undefined') {\nrequire.undef(\"plotly\");\nrequirejs.config({\n    paths: {\n        'plotly': ['https://cdn.plot.ly/plotly-2.16.1.min']\n    }\n});\nrequire(['plotly'], function(Plotly) {\n    window._Plotly = Plotly;\n});\n}\n</script>\n\n"
      ]
    }
  }
}