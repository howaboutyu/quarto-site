---
title: My document
format: html
reference-location: margin
jupyter: python3
bibliography: ../../references.bib

html-math-method: katex


editor:
  render-on-save: true
#
# can have png banner : https://quarto.org/docs/authoring/title-blocks.html
---




## Introductiosn
Given a set of images for a scene, we can construct a representation of the scene using a neural network through integration - how awesomely cool is that! The NeRF model [@Mildenhall2020] is a neural network which integrates many different rays coming towards the camera that forms an image. It uses a multi-layered perception (or MLP),  that can be trained to represent a scene from a set of images, and after training we can synthesis an image taken from a new angle.

When I first read the NeRF paper, I was quite excited about it. I was thinking that this is a great way to learn about 3D reconstruction, and also learn about how neural networks are used for scene reconstruction. I was also thinking that this is a great way to learn about JAX, since I have only been using Tensorflow and PyTorch.

Before coding up NeRF, understanding the paper is important. The paper is quite short, and it is easy to understand. I would recommend reading the paper first, and then come back to this blog. I've also listed all of the reference materials I've used for this project.

## The Network 

The NeRF model represents a scene as a neural network, where at each 3D poisition $\vec{x} \in \mathbb{R}^3$ and viewing direction $\vec{d} \in \mathbb{R}^3$,  and the output of the model is the color (RGB) $\vec{c}$ and the density $\sigma$ for that point in space. Thus we have: 

$$
F_\Theta : ( \vec{x}, \vec{d}) \rightarrow (\vec{c}, \sigma)
$$

The network $F_\Theta$ is an Multi-Layer Perceptron (MLP); the network is super simple illustrated in @fig-nerf-model, it is just a bunch of linear layers with ReLU activations. But, the authors' designed the network to ensured that the volume density $\sigma$ is view independent ($\partial{\sigma} / \partial{\vec{d}} = 0$) by setting the directional input $\vec{d}$ after the $\sigma$ output.

![
The NeRF model @Mildenhall2020. The blue blocks represents MLP layers. Black arrows indicates ReLU activation layer, orange arrow indicates no activation and dashed black line indicates sigmoid activiation.
](nerf_model.png "Title: An elephant"){#fig-nerf-model}


### Camera ray and position encoding


#### Camera ray 

Before we can train the network, we need to generate the rays that will be used to train the network. The rays are generated from the camera, and the camera is defined by the extrinsic parameters - rotation and translation matrices - $[R | T]$, the focal length, and the near/far distances. The origin $\vec{o}$ would be the camera translation vector for that image.

The vector $\vec{d}$, can be thought of as $\vec{d}= R\vec{d}_o$, where $\vec{d}_o$ is the direction vector $\vec{d}_o = [x, y, -f]$ using the pin-hole camera model [cite],  the magnitude doesn't maktter so $\vec{d}_o = [x/f, y/f, -1]$


#### Positional encoding

The positional encoding is a simple way to encode the 3D position $\vec{x}$ into a vector. The authors used a sine and cosine function to encode the position to enable the MLP to capture higher frequencey functions. The encoding is given by:

$$
\gamma(x) = (sin(2^0\pi x), cos(2^0 x), \dots, sin(2^{L-1}\pi x), cos(2^{L-1} x))
$$

where $x$ can be either the position or the direction, and $L$ is the number of frequencies. The authors used $L_{position}=10$  and $L_{direction}=4$.



#### Model in JAX

Using [Flax](https://github.com/google/flax), we can easily define the model in JAX. The code is shown below:

```{python}
# code-fold: true
import jax
import flax.linen as nn
import jax.numpy as jnp


class Model(nn.Module):

  @nn.compact
  def __call__(self, position, direction):
    x =  position

    for i in range(7):
        x = nn.Dense(256, name=f'fc{i + 1}')(x)
        x = nn.relu(x)

        # Concat x with original input 
        if i == 4:
            x = jnp.concatenate([x, position], -1)

    x = nn.Dense(256, name=f'fc{8}_linear')(x)

    vol_density = nn.Dense(1, name=f'fc{8}_sigmoid')(x)

    # Create an output for the volume density that is view independent
    # and > 0 by using a relu activation function 
    vol_density = jax.nn.relu(vol_density)

    # Concat direction information after the volume density
    x = jnp.concatenate([x, direction], -1)
    x = nn.Dense(128, name='fc_128')(x)
    x = nn.relu(x)
    x = nn.Dense(3, name='fc_f')(x)

    # rgb color is between 0 and 1
    rgb = nn.sigmoid(x)
    return rgb, vol_density


L_position = 10 
L_direction = 4 
random_position = jnp.ones((1, L_position * 6 + 3))
random_direction = jnp.ones((1, L_direction * 6 + 3))


```

::: {.callout-tip}
Use the code below to print the model architecture
```python
print(Model().tabulate(jax.random.PRNGKey(0), random_position, random_direction))
```
:::


### How does the network work?

Imagine a line or ray pointed towards the camera, with equation $\vec{r}(t) = \vec{o} +t \vec{d}$; where $\vec{o}$ is the origin point on the ray and $t$ is a parameter controls how far along the ray we are at. The authors split the ray into $N$ segments, where each segment will have length equaled to $\delta_i = t_{i+1}-t_i$. At each segment along the ray, we can find the color and density of segment $i$ approximated by the neural network $F_\Theta(\vec{r}(t_i), \vec{d_i}) \rightarrow (\vec{c}_i, \sigma_i)$. We can get the expected color that would appear on our image by using a set of approximation equations, outlined below:

$$
\hat{C}(\vec{r}) = \sum_{i=1}^{N} T_i a_i \vec{c}_i 
$$

where, 

$$
a_i = (1- \text{exp}(-\sigma_i \delta_i))
$$


and,
$$
T_i=\text{exp}\left( -\sum_{j=1}^{i-1} \sigma_j \delta_j \right)
$$

$T_i$ for a segment, is the accumulated transmittance upto that point, which can be thought of as the the probability that the ray has not hit anything up to that point, so every point before a solid object along the ray, $T_i$ should be $1$. If the ray encounters some solid object, then $T_i$ would start to decrease because the volume density $\sigma$ starts to increase.  We can see that the alpha values $a_i$ for low volume density becomes $0$ which ignores that colour in the integral. What an interesting relationship!



The above equations can be written with `jnp`, a JAX version of `numpy` as:

```python

...
# get the accumulated transmittance
T_i = jnp.cumsum(jnp.squeeze(sigma) * t_delta + 1e-10, -1)
T_i = jnp.insert(T_i, 0, jnp.zeros_like(T_i[...,0]),-1)
T_i = jnp.exp(-T_i)[..., :-1]

# get the alpha values
alpha = (1.-jnp.exp(-sigma*t_delta[..., jnp.newaxis]))

weights = T_i[..., jnp.newaxis] * alpha
c_array = weights * rgb
c_sum =jnp.sum(c_array, -2)
return c_sum
```


## Hierarchical volume sampling

Free space and solid objects don't contribute to the final color, so we can sample the volume more densely around points along the ray that have a larger weight - this is called hierarchical volume sampling.


First sample a coarse set of points, then they sample a fine set of points around the coarse set. 



This method is used to sample points closer to the surface of the objects. They first sample $N_c$ locations using stratified sampling, then they sample a second set $N_f$ using inverse transform sampling of the $\text{PDF}$ created using $w_i = T_i(1-\exp(-\sigma_i \delta_i))$, or

$$
f_X(x) = \frac{T_x(1-\exp(-\sigma_x \delta_x))}{k} \quad 1 < x < N_c 
$$


where $k=\sum_j^{N_c} w_j$ is the normalizing factor, this is the ensure that the area under the $\text{PDF}$ is 1. Let's plot the PDF for a ray (with 128 points) at the center of the image, and also verify that the area is 1 under the curve.

```{python}
#| code-fold: true
import numpy as np
weights = np.squeeze(np.load('weights.npy'))

k = np.sum(weights) 
# normalize
weights = weights/k
dx = 1. 
x=np.arange(0, 128)

import plotly.express as px
fig = px.line(x=x, y=weights, \
    labels={
        'x': "counter along x",
        'y': "probability"
    }, title='PDF of weights')
fig.show()

# area
area = np.sum(weights * dx)
print(f'Area under the curve is :{area}')
```


### Inverse transform sampling

The idea behind inverse transform sampling is to use the inverse of the $\text{CDF}$ to generate random numbers for a probability distribution. The $\text{CDF}$ for a random variable $X$ is $F_X(x) = P(X\leq x)$. Then, generate random numbers from a uniform distribution  $Z \sim \text{uni}(0, 1)$, using this with the inverse $\text{CDF}$ (or  $F^{-1}_X(Z)$) to get samples from the original distribution.

Let's plot the CDF and the inverse cdf.
```{python}
#| code-fold: true
cdf = np.cumsum(weights)
fig = px.line(x=x, y=cdf, title="CDF ", labels={
    'x': 'counter along x',
    'y': 'probability'
})
fig.show()

fig = px.line(x=cdf, y=x, title="Inverse CDF ", labels={
    'y': 'counter along x',
    'x': 'probability'
})
fig.show()



```

We can see that if we sample the inverse cdf - with a uniform distribution - the value of the y-axis (or the counter along the x-axis) would fall around 55, which corresponds to the section with the highest probability density of the pdf plot. 

Let's plot a histogram of 1000 sampled counter using inverse transform sampling method. 
```{python}
#| code-fold: true 
import jax.numpy as jnp 
import jax
Z = jax.random.uniform(key=jax.random.PRNGKey(0), shape=[1000])
cdf = jnp.array(cdf)

def inverse_cdf(prob, cdf):
    x = jnp.linspace(2, 6, 128)
    abs_diff = jnp.abs(prob[..., jnp.newaxis] - cdf[jnp.newaxis, ...])
    argmin = jnp.argmin(abs_diff, -1)
    return x[argmin]

X = inverse_cdf(Z, cdf)

 
fig = px.histogram(x=X, labels={'x':'distance along the camera ray '})
fig.show()
```

```{python}
t = np.load('../../t.npy')
print(t.shape)

fig = px.histogram(x=t[50,50])
fig.show()
```

## Training


